import json
import logging
import threading
import traceback
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path

from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from common.config import inject
from common.helpers import notify_task_completion

logger = logging.getLogger(__name__)


@inject(
    workflow_conf_key="workflowId",
    read_params=["s3_location_bucket", "nas_folder_path", "__image_group_paths"],
    method="GET"
)
def copy_data_to_nas(**context):
    task_instance = context.get("task_instance") or context.get("ti")
    task_name = task_instance.task_id

    try:
        dag_run = context.get("dag_run")
        workflow_id = dag_run.conf.get("workflowId") if dag_run else "unknown"
        logger.info(f"[{task_name}] Starting copy_data_to_nas task with workflowId: {workflow_id}")

        s3_location_bucket = context["s3_location_bucket"]
        nas_folder_path = context["nas_folder_path"]
        image_group_paths = context["__image_group_paths"]
        nas_root_path = context.get("nas_root_path")
        hook = S3Hook(aws_conn_id="aws_default")

        task_inputs = []
        for item in image_group_paths:
            folder_id = item.get("s3FolderId")
            file_name = item.get("fileName")
            if folder_id and file_name:
                task_inputs.append({
                    "s3_location_bucket": s3_location_bucket,
                    "nas_folder_path": nas_folder_path,
                    "folder_id": folder_id,
                    "file_name": file_name,
                    "workflow_id": workflow_id
                })

        successful_downloads = []
        failed_downloads = []

        def download_one(item):
            thread_name = threading.current_thread().name
            logger.info(f"nas_root_path={nas_root_path}")
            logger.info(item)
            try:
                s3_key = f"{item['folder_id']}{item['file_name']}"
                dest_path = Path(nas_root_path) / item["nas_folder_path"] / item["s3_location_bucket"] / item[
                    "folder_id"] / item["file_name"]
                logger.info(f"dest_path={dest_path}")

                if dest_path.is_file():
                    s3_object = hook.get_key(bucket_name=item["s3_location_bucket"], key=s3_key)
                    s3_size = s3_object["ContentLength"]
                    local_size = dest_path.stat().st_size

                    if s3_size == local_size:
                        logger.info(f"[THREAD: {thread_name}] Skipping {s3_key} — already downloaded and size matches.")
                        return {"status": "skipped", "file": str(dest_path), "thread": thread_name}
                    else:
                        logger.warning(
                            f"[THREAD: {thread_name}] Size mismatch for {s3_key} — local: {local_size}, S3: {s3_size}. Redownloading...")

                dest_path.parent.mkdir(parents=True, exist_ok=True)

                hook.download_file(
                    key=s3_key,
                    bucket_name=item["s3_location_bucket"],
                    local_path=str(dest_path.parent),
                    preserve_file_name=True,
                    use_autogenerated_subdir=False
                )

                logger.info(f"[THREAD: {thread_name}] Downloaded {s3_key} to {dest_path}")
                return {"status": "success", "file": str(dest_path), "thread": thread_name}

            except Exception as e:
                logger.error(traceback.format_exc())
                logger.error(
                    f"\n[THREAD: {thread_name}] Failed to download file\n"
                    f"→ JSON: {json.dumps(item, indent=2)}\n"
                    f"→ Exception: {type(e).__name__} - {str(e)}"
                )
                return {"status": "failed", "file": item["file_name"], "thread": thread_name}

        logger.info(f"Starting parallel download with {len(task_inputs)} files")

        with ThreadPoolExecutor(max_workers=16) as executor:
            futures = [executor.submit(download_one, item) for item in task_inputs]
            for future in as_completed(futures):
                result = future.result()
                if result["status"] == "success":
                    successful_downloads.append(result)
                elif result["status"] == "failed":
                    failed_downloads.append(result)

        logger.info(f"Download completed. Success: {len(successful_downloads)}, Failed: {len(failed_downloads)}")

        try:
            notify_task_completion(
                workflow_id=workflow_id,
                task_name=task_name,
                payload={"copy_data_to_nas": "completed"}
            )
            logger.info(f"[{task_name}] Task completed successfully. Downloaded {len(successful_downloads)} files.")
        except Exception as notify_error:
            logger.error(f"[{task_name}] Notification failed: {type(notify_error).__name__} - {str(notify_error)}")
            raise

    except Exception as e:
        logger.error(f"[{task_name}] Task failed copy data to nas: {type(e).__name__} - {str(e)}")
        error_payload = {
            "workflowId": workflow_id,
            "taskName": task_name,
            "errorMessage": str(e)
        }
        task_instance.xcom_push(key=task_name, value=error_payload)
        raise
